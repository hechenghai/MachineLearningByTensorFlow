# 模型为什么看起来是收敛的 #

----------



&ensp;&ensp;&ensp;&ensp;我们知道在优化算法中，局部最优点很难踩到，而且鞍点也极不稳定，很容易跳出去(原因在[训练神经网络时batch大小的影响](https://github.com/hechenghai/MachineLearningByTensorFlow/blob/master/ProblemsWhileProgramming/%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%97%B6batch%E5%A4%A7%E5%B0%8F%E7%9A%84%E5%BD%B1%E5%93%8D.md)中已经说过了)。但是很多时候，我们的模型的效果仍然是比较稳定的，看起来像是收敛的。


----------
&ensp;&ensp;&ensp;&ensp; 一开始，我以为是学习率太大了，导致在鞍点附近震荡。

&ensp;&ensp;&ensp;&ensp;首先鞍点不像最优点那样容易震荡，而且哪怕不断减小学习率继续让模型收敛，此时计算output层或者后几层梯度向量的长度时，发现它依然离很远。

&ensp;&ensp;&ensp;&ensp;其实，在高维空间里(深度学习问题上)真正可怕的不是局部最优点也不是鞍点，而是一些特殊地形。比如大面积的平坦区域：

![](https://i.imgur.com/KPOnf1G.png)

&ensp;&ensp;&ensp;&ensp;在平坦区域，虽然导数不为0，但是却不大。虽然是在不断下降但是路程却非常长。对于优化算法来说，它需要走很多很多步才有可能走过这一片平坦区域。甚至在这段地形的二阶导数过于特殊的情况下，一阶优化算法走无穷多步也走不出去。

&ensp;&ensp;&ensp;&ensp;所以相比栽倒最优点和鞍点上，优化算法更有可能栽到这样类似平坦的地形中(如果这个平坦区域又是一个高原地带，即loss值很高的地带，那更糟糕)。更糟糕的是，由于高维地形难以可视化，还有很多复杂的未知地形会导致假收敛，一旦陷入到这些危险地形中，几乎是无解的。

**&ensp;&ensp;&ensp;&ensp;所以说，在深度学习中，与其担忧陷入局部最优点怎么跳出来，更不如好好考虑：**

1. 如何去设计一个尽量没有“平坦区”等危险地形的loss空间，即着手于loss函数的设计以及深度学习模型的设计；
2. 尽量让模型的初始化远离空间中的危险地带，让最优化游戏开始于简单模式，即着手于模型参数的初始化策略；
3. 让最优化过程更智能一点，该加速冲时加速冲，该大胆跳跃时就大胆跳，该慢慢踱步时慢慢走，对危险地形有一定的判断力，如梯度截断策略；
4. 开外挂，本来下一步要走向死亡的，结果被外挂给拽回安全区，如*batch normalization*策略等。