# 激活函数对比分析 #

----------

## 什么是激活函数 ##

如下图，在神经元中，输入的inputs通过加权，求和后，还需要经过一个函数，这个函数就是激活函数。

![](https://ask.qcloudimg.com/http-save/yehe-1000059/d4a315hy6y.png?imageView2/2/w/1620)

如果没有激活函数，那么每一层的输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。

如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多非线性模型中。

## Sigmoid 激活函数 ##

公式：

<img src="https://latex.codecogs.com/gif.latex?f\left&space;(&space;x&space;\right&space;)&space;=&space;\frac{exp\left&space;(&space;x&space;\right&space;)}{1&plus;exp\left&space;(&space;x&space;\right&space;)}&space;=&space;\frac{1}{1&plus;exp\left&space;(&space;-x&space;\right&space;)}" title="f\left ( x \right ) = \frac{exp\left ( x \right )}{1+exp\left ( x \right )} = \frac{1}{1+exp\left ( -x \right )}" />

曲线：

![](https://ask.qcloudimg.com/http-save/yehe-1000059/vvxk1a0r9w.png?imageView2/2/w/1620)

Sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)。

例如，输入向量值为[-0.5, 1.2, -0.1, 2.4]，输出向量为[-0.5, 1.2, 09.1, 2.4]

Sigmoid函数可以将输入值映射为概率，但是由于Sigmoid函数在计算输入向量时，分别独立地计算输入向量的每个值，可以允许多种可能性并存的情况，所以可以解决多标签的分类问题。

Sigmoid函数的优点：

Sigmoid = 多标签分类问题 = 多个正确答案 = 非独占输出(例如胸部X光检查、住院)

Sigmoid函数的缺点：

1. 激活函数计算量大，反向传播求导误差梯度时，求导涉及除法；
2. 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练

那为什么会出现梯度消失的现象呢？

反向传播算法中，要对激活函数求导，sigmoid的导数表达式为：

<img src="https://latex.codecogs.com/gif.latex?{\phi}'&space;=&space;\phi\left&space;(&space;x&space;\right&space;)\left&space;(&space;1-\phi\left&space;(&space;x&space;\right&space;)&space;\right&space;)" title="{\phi}' = \phi\left ( x \right )\left ( 1-\phi\left ( x \right ) \right )" />
sigmoid的导数图形为：

![](https://ask.qcloudimg.com/http-save/yehe-1000059/rtp1i3q7ha.png?imageView2/2/w/1620)

由上图可以看出，导数从0开始达到最高点(0.25)之后，很快就又趋近于0了，易造成“梯度消失”现象。


## Softmax 激活函数 ##

公式：

<img src="https://latex.codecogs.com/gif.latex?f\left&space;(&space;x_i&space;\right&space;)&space;=&space;\frac{exp\left&space;(&space;x_i&space;\right&space;)}{\sum&space;exp\left&space;(&space;x_j&space;\right&space;)}" title="f\left ( x_i \right ) = \frac{exp\left ( x_i \right )}{\sum exp\left ( x_j \right )}" />

曲线：

![](https://img-blog.csdnimg.cn/20181204232456945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4MDcxMzY=,size_16,color_FFFFFF,t_70)

Softmax函数是Sigmoid函数的扩展，可以直接用于解决多分类问题，而且类与类之间是互斥的，一个输入只能归为一类，当归为一类的概率增大时，归为其他类的概率必然会减小，而Sigmoid函数


